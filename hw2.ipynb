{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JacobyoonDH/GAMA-Floodsim-BagongSilangan/blob/master/hw2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rf2KB3D1okyq"
      },
      "source": [
        "> ### EEE6503-01: Computer Vision\n",
        "\n",
        "# Assignment \\# 2: Mask R-CNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7VfrAswVJgWX"
      },
      "source": [
        "**<div style=\"text-align: right\"> Due date: Nov. 1st, 2024. </div>**\n",
        "**<div style=\"text-align: right\"> Please upload your file @ learnus by 9:00 PM. </div>**\n",
        "<div style=\"text-align: right\"> Lee Geon(이건): <a href=\"mailto:geon.lee@yonsei.ac.kr\">geon.lee@yonsei.ac.kr</a>\n",
        "<div style=\"text-align: right\"> Sanghoon Lee(이상훈): <a href=\"mailto:shoon.lee@yonsei.ac.kr\">shoon.lee@yonsei.ac.kr</a> </div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z_kEzj9jokyr"
      },
      "source": [
        "### *Assignment Instructions:*\n",
        "1. Write a program implementing a particular algorithm to solve a given problem.\n",
        "2. You can use both Korean and English for your report.\n",
        "3. **Analyze the algorithm, theoretically and empirically.**\n",
        "4. **Report your results.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XhLkyjyJokyr"
      },
      "source": [
        "<h2><span style=\"color:blue\">[2024314217] [윤두휘]</span> </h2>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "eYv-XwJRokyr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b10eae25-ac63-49a4-ff45-fb23c83c3607"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This code is written at 2024-10-25 04:20:53.918390\n"
          ]
        }
      ],
      "source": [
        "import datetime\n",
        "print(\"This code is written at \" + str(datetime.datetime.now()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fzjdzvb_okys"
      },
      "source": [
        "The objective of this assignment is to fine-tune a [Mask R-CNN](https://arxiv.org/abs/1703.06870) model using a [PennFudanPed](https://www.cis.upenn.edu/~jshi/ped_html/) dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nu_1dSbl_93L"
      },
      "source": [
        "Do not import other functions using \"wget\", unless otherwise specified."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "QVODldsvokys"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import math\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import sklearn.datasets\n",
        "import os\n",
        "\n",
        "torch.manual_seed(100)\n",
        "np.random.seed(100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "jsuxybcirwcM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b72b4e97-c46f-47ee-e4b6-abccc4240ac3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "os.system(\"wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/utils.py\")\n",
        "os.system(\"wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/transforms.py\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x8IIn-lB3QHB",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4de10ce5-137a-4ac3-8aa9-08886a7e1a11"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  PennFudanPed.zip\n",
            "replace PennFudanPed/added-object-list.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ],
      "source": [
        "#!wget https://www.cis.upenn.edu/~jshi/ped_html/PennFudanPed.zip -P data\n",
        "!cd ./data && unzip PennFudanPed.zip\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wOV3xRnfokys"
      },
      "source": [
        "Step 1: Load a dataset\n",
        "\n",
        "You can adjust the batch size if your GPU memory is not enough."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "IBVPHo_w3hfo"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "\n",
        "from torchvision.io import read_image\n",
        "from torchvision.ops.boxes import masks_to_boxes\n",
        "from torchvision import tv_tensors\n",
        "from torchvision.transforms.v2 import functional as F\n",
        "\n",
        "\n",
        "class PennFudanDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, root, transforms):\n",
        "        self.root = root\n",
        "        self.transforms = transforms\n",
        "        # load all image files, sorting them to\n",
        "        # ensure that they are aligned\n",
        "        self.imgs = list(sorted(os.listdir(os.path.join(root, \"PNGImages\"))))\n",
        "        self.masks = list(sorted(os.listdir(os.path.join(root, \"PedMasks\"))))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # load images and masks\n",
        "        img_path = os.path.join(self.root, \"PNGImages\", self.imgs[idx])\n",
        "        mask_path = os.path.join(self.root, \"PedMasks\", self.masks[idx])\n",
        "        img = read_image(img_path)\n",
        "        mask = read_image(mask_path)\n",
        "        # instances are encoded as different colors\n",
        "        obj_ids = torch.unique(mask)\n",
        "        # first id is the background, so remove it\n",
        "        obj_ids = obj_ids[1:]\n",
        "        num_objs = len(obj_ids)\n",
        "\n",
        "        # split the color-encoded mask into a set\n",
        "        # of binary masks\n",
        "        masks = (mask == obj_ids[:, None, None]).to(dtype=torch.uint8)\n",
        "\n",
        "        # get bounding box coordinates for each mask\n",
        "        boxes = masks_to_boxes(masks)\n",
        "\n",
        "        # there is only one class\n",
        "        labels = torch.ones((num_objs,), dtype=torch.int64)\n",
        "\n",
        "        image_id = idx\n",
        "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
        "        # suppose all instances are not crowd\n",
        "        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n",
        "\n",
        "        # Wrap sample and targets into torchvision tv_tensors:\n",
        "        img = tv_tensors.Image(img)\n",
        "\n",
        "        target = {}\n",
        "        target[\"boxes\"] = tv_tensors.BoundingBoxes(boxes, format=\"XYXY\", canvas_size=F.get_size(img))\n",
        "        target[\"masks\"] = tv_tensors.Mask(masks)\n",
        "        target[\"labels\"] = labels\n",
        "        target[\"image_id\"] = image_id\n",
        "        target[\"area\"] = area\n",
        "        target[\"iscrowd\"] = iscrowd\n",
        "\n",
        "        if self.transforms is not None:\n",
        "            img, target = self.transforms(img, target)\n",
        "\n",
        "        return img, target\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.imgs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "4M4YRt8aViK8"
      },
      "outputs": [],
      "source": [
        "from torchvision.transforms import v2 as T\n",
        "\n",
        "def get_transform(train):\n",
        "    transforms = []\n",
        "    if train:\n",
        "        transforms.append(T.RandomHorizontalFlip(0.5))\n",
        "    transforms.append(T.ToDtype(torch.float, scale=True))\n",
        "    transforms.append(T.ToPureTensor())\n",
        "    return T.Compose(transforms)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "r3mCtofgVj4T",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "outputId": "eaab2211-29ee-4ee2-a68d-d821bfb639ab"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'data/PennFudanPed/PNGImages'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-b4130cb5a417>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPennFudanDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/PennFudanPed'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m data_loader = torch.utils.data.DataLoader(\n\u001b[1;32m      5\u001b[0m     \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-85a2e28588eb>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, transforms)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;31m# load all image files, sorting them to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;31m# ensure that they are aligned\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"PNGImages\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmasks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"PedMasks\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/PennFudanPed/PNGImages'"
          ]
        }
      ],
      "source": [
        "import utils\n",
        "\n",
        "dataset = PennFudanDataset('data/PennFudanPed', get_transform(train=True))\n",
        "data_loader = torch.utils.data.DataLoader(\n",
        "    dataset,\n",
        "    batch_size=2,\n",
        "    shuffle=True,\n",
        "    num_workers=4,\n",
        "    collate_fn=utils.collate_fn\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WNTObm87VXuo"
      },
      "outputs": [],
      "source": [
        "# train on the GPU or on the CPU, if a GPU is not available\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "# our dataset has two classes only - background and person\n",
        "num_classes = 2\n",
        "# use our dataset and defined transformations\n",
        "dataset = PennFudanDataset('data/PennFudanPed', get_transform(train=True))\n",
        "dataset_test = PennFudanDataset('data/PennFudanPed', get_transform(train=False))\n",
        "\n",
        "# split the dataset in train and test set\n",
        "indices = torch.randperm(len(dataset)).tolist()\n",
        "dataset = torch.utils.data.Subset(dataset, indices[:-50])\n",
        "dataset_test = torch.utils.data.Subset(dataset_test, indices[-50:])\n",
        "\n",
        "# define training and validation data loaders\n",
        "data_loader = torch.utils.data.DataLoader(\n",
        "    dataset,\n",
        "    batch_size=2,\n",
        "    shuffle=True,\n",
        "    num_workers=4,\n",
        "    collate_fn=utils.collate_fn\n",
        ")\n",
        "\n",
        "data_loader_test = torch.utils.data.DataLoader(\n",
        "    dataset_test,\n",
        "    batch_size=1,\n",
        "    shuffle=False,\n",
        "    num_workers=4,\n",
        "    collate_fn=utils.collate_fn\n",
        ")\n",
        "\n",
        "\n",
        "# use our dataset and defined transformations\n",
        "dataset = PennFudanDataset('data/PennFudanPed', get_transform(train=True))\n",
        "dataset_test = PennFudanDataset('data/PennFudanPed', get_transform(train=False))\n",
        "\n",
        "# split the dataset in train and test set\n",
        "indices = torch.randperm(len(dataset)).tolist()\n",
        "dataset = torch.utils.data.Subset(dataset, indices[:-50])\n",
        "dataset_test = torch.utils.data.Subset(dataset_test, indices[-50:])\n",
        "\n",
        "# define training and validation data loaders\n",
        "data_loader = torch.utils.data.DataLoader(\n",
        "    dataset,\n",
        "    batch_size=2,\n",
        "    shuffle=True,\n",
        "    num_workers=4,\n",
        "    collate_fn=utils.collate_fn\n",
        ")\n",
        "\n",
        "data_loader_test = torch.utils.data.DataLoader(\n",
        "    dataset_test,\n",
        "    batch_size=1,\n",
        "    shuffle=False,\n",
        "    num_workers=4,\n",
        "    collate_fn=utils.collate_fn\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jsn1Jj7Ookyt"
      },
      "source": [
        "Step 2: Load a model\n",
        "\n",
        "Load a Mask R-CNN model with a backbone of FPN pretrained on a COCO dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oIvk3U-AJgWZ"
      },
      "outputs": [],
      "source": [
        "import torchvision\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
        "\n",
        "def get_model_instance_segmentation(num_classes):\n",
        "    # load an instance segmentation model pre-trained on COCO\n",
        "    model = torchvision.models.detection.maskrcnn_resnet50_fpn(weights=\"DEFAULT\")\n",
        "\n",
        "    # get number of input features for the classifier\n",
        "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "    # replace the pre-trained head with a new one\n",
        "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
        "\n",
        "    # now get the number of input features for the mask classifier\n",
        "    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
        "    hidden_layer = 256\n",
        "    # and replace the mask predictor with a new one\n",
        "    model.roi_heads.mask_predictor = MaskRCNNPredictor(\n",
        "        in_features_mask,\n",
        "        hidden_layer,\n",
        "        num_classes\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "# num_classes which is user-defined\n",
        "num_classes = 2  # 1 class (person) + background\n",
        "\n",
        "model = get_model_instance_segmentation(num_classes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2QubP_zioAWF"
      },
      "source": [
        "Write your own MultiScaleRoIAlign code in \"model.roi_heads.box_roi_pool\", instead of using **torchvision.ops.MultiScaleRoIAlign , torchvision.ops.RoIAlign, and torchvision.ops.roi_align**."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class CustomMultiScaleRoIAlign(nn.Module):\n",
        "    def __init__(self, output_size, sampling_ratio=2):\n",
        "        super(CustomMultiScaleRoIAlign, self).__init__()\n",
        "        self.output_size = output_size\n",
        "        self.sampling_ratio = sampling_ratio\n",
        "\n",
        "    def forward(self, features, proposals, image_shapes):\n",
        "        # This implementation assumes features from multiple levels and combines them\n",
        "        # features: list of tensors with different spatial sizes\n",
        "        pooled_features = []\n",
        "        for proposal in proposals:\n",
        "            # Determine appropriate feature map based on the scale of the proposal\n",
        "            # Here we use a simple heuristic: select the feature map based on proposal size\n",
        "            best_level = self.select_feature_map_level(proposal, features)\n",
        "            feature_map = features[best_level]\n",
        "\n",
        "            # Extract features for the current proposal\n",
        "            roi_feature = self.roi_pool(feature_map, proposal)\n",
        "            pooled_features.append(roi_feature)\n",
        "\n",
        "        # Stack all pooled features into a single tensor\n",
        "        pooled_features = torch.stack(pooled_features, dim=0)\n",
        "        return pooled_features\n",
        "\n",
        "    def select_feature_map_level(self, proposal, features):\n",
        "        # Heuristic to select feature level based on proposal size\n",
        "        # Adjust this heuristic to suit your requirement\n",
        "        area = (proposal[2] - proposal[0]) * (proposal[3] - proposal[1])\n",
        "        if area < 32 * 32:\n",
        "            return 0\n",
        "        elif area < 64 * 64:\n",
        "            return 1\n",
        "        elif area < 128 * 128:\n",
        "            return 2\n",
        "        else:\n",
        "            return 3\n",
        "\n",
        "    def roi_pool(self, feature_map, proposal):\n",
        "        # Extract a region of interest from the feature map and perform adaptive max pooling\n",
        "        x1, y1, x2, y2 = proposal.int()\n",
        "        roi = feature_map[:, :, y1:y2, x1:x2]\n",
        "        pooled_roi = F.adaptive_max_pool2d(roi, self.output_size)\n",
        "        return pooled_roi\n"
      ],
      "metadata": {
        "id": "TT0w8YLbLX2C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d0nq5fxbl_q4",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "print(model.roi_heads.box_roi_pool)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "59IIGK8IofA8"
      },
      "outputs": [],
      "source": [
        "model.roi_heads.box_roi_pool = CustomMultiScaleRoIAlign(output_size=(7, 7))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7SJwIFPMR4Ah"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3fubupODokyu"
      },
      "source": [
        "Step 3: Train the model\n",
        "\n",
        "Configurations for training the model is shown below. Please refer to [paper](https://arxiv.org/abs/1703.06870) for loss functions.\n",
        "\n",
        "* Optimization: Stochastic gradient descent (SGD)\n",
        "* Learning rate: 0.005 (You can change the learning rate)\n",
        "* The number of epochs: 5 (You can change the number of epochs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tQcqslHNo6RG"
      },
      "outputs": [],
      "source": [
        "# construct an optimizer\n",
        "params = [p for p in model.parameters() if p.requires_grad]\n",
        "optimizer = torch.optim.SGD(\n",
        "    params,\n",
        "    lr=0.005,\n",
        "    momentum=0.9,\n",
        "    weight_decay=0.0005\n",
        ")\n",
        "\n",
        "# and a learning rate scheduler\n",
        "lr_scheduler = torch.optim.lr_scheduler.StepLR(\n",
        "    optimizer,\n",
        "    step_size=3,\n",
        "    gamma=0.1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zUaKwfnbSkIR"
      },
      "outputs": [],
      "source": [
        "# Train the model\n",
        "import time\n",
        "\n",
        "# Define the number of epochs and device\n",
        "epochs = 5\n",
        "model.train()\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    start_time = time.time()\n",
        "    epoch_loss = 0.0\n",
        "\n",
        "    for images, targets in data_loader:\n",
        "        images = [img.to(device) for img in images]\n",
        "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "\n",
        "        # Zero out the gradient\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        loss_dict = model(images, targets)\n",
        "\n",
        "        # Total loss calculation\n",
        "        losses = sum(loss for loss in loss_dict.values())\n",
        "\n",
        "        # Backward pass and optimizer step\n",
        "        losses.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Accumulate epoch loss\n",
        "        epoch_loss += losses.item()\n",
        "\n",
        "    # Update learning rate\n",
        "    lr_scheduler.step()\n",
        "\n",
        "    end_time = time.time()\n",
        "    print(f\"Epoch [{epoch+1}/{epochs}], Loss: {epoch_loss:.4f}, Time: {end_time - start_time:.2f} sec\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gw3tZ_FGokyu"
      },
      "source": [
        "Step 4: Test the model\n",
        "\n",
        "Evaluate the detection performance using the test_loader without using **torchvision.ops.box_iou**. Write your own evalutaion code.\n",
        "* Box-level mAP: a mean value of Average Precision (AP) @ [box IoU=0.50:0.05:0.95].\n",
        "* Mask-level mAP: a mean value of Average Precision (AP) @ [mask IoU=0.50:0.05:0.95]."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QO6WODYjJgWa"
      },
      "outputs": [],
      "source": [
        "# write your own code for computing IoU(Intersection over union)\n",
        "# given ground-truth bounding box and predicted box.\n",
        "def compute_bbox_IoU(gt_bbox, pred_bbox):\n",
        "    #############\n",
        "    # Code here #\n",
        "    #############\n",
        "\n",
        "# write your own code for computing IoU(Intersection over union)\n",
        "# given ground-truth mask and predicted mask.\n",
        "def compute_mask_IoU(gt_mask, pred_mask):\n",
        "    #############\n",
        "    # Code here #\n",
        "    #############\n",
        "\n",
        "# write your own code for computing AP(Average Precision)\n",
        "# given ground-truth bounding boxes, predicted boxes, and scores of predicted boxes.\n",
        "def compute_bbox_AP(gt_bboxes, pred_bboxes, scores, iou_threshold)\n",
        "    #############\n",
        "    # Code here #\n",
        "    #############\n",
        "\n",
        "# write your own code for computing AP(Average Precision)\n",
        "# given ground-truth mask, predicted mask, and scores of maskes.\n",
        "def compute_mask_AP(gt_maskes, pred_maskes, scores, iou_threshold)\n",
        "    #############\n",
        "    # Code here #\n",
        "    #############"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vd3ZRO1wSqNk"
      },
      "outputs": [],
      "source": [
        "## Test the model\n",
        "\n",
        "#############\n",
        "# Code here #\n",
        "#############"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H10ADUY0okyu"
      },
      "source": [
        "Step 5: Visualization\n",
        "\n",
        "Visualize an example image along with corresponding box and mask predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mssCVel7p_oX"
      },
      "outputs": [],
      "source": [
        "## Visualize an example of predictions\n",
        "\n",
        "#############\n",
        "# Code here #\n",
        "#############"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ShblEXKJgWa"
      },
      "source": [
        "Step 6: Discussion\n",
        "\n",
        "Train and evaluate the model by replacing the ROIAlign operator with the ROIPool operator. Compare the mAP results without using **torchvision.ops.RoIPool, and torchvision.ops.roi_pool**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6tzeoE2RJgWa"
      },
      "source": [
        "<!-- Step 6: Discussion\n",
        "\n",
        "Train and evaluate the detection performance of the model by replacing the ROIAlign operator with the ROIPool operator. Compare the results without using **torchvision.ops.RoIPool, and torchvision.ops.roi_pool**. -->"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}